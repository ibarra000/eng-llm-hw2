{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook focuses on performing Supervised Fine-Tuning (SFT) on the base model used in Part 1. We will use the `mbpp-rkt-train` dataset to fine-tune the model on the task of Racket code generation. The key steps are:\n",
    "\n",
    "1.  **Configuration**: Set up different hyperparameter configurations for our training experiments.\n",
    "2.  **Data Preparation**: Load and format the training dataset into a prompt-completion structure.\n",
    "3.  **W&B Logging**: Integrate Weights & Biases to log metrics like loss and learning rate.\n",
    "4.  **Training**: Implement a training loop using PyTorch and the `transformers` library.\n",
    "5.  **Model Saving**: Save the fine-tuned model and tokenizer for evaluation in Part 3.\n",
    "\n",
    "We will run at least two experiments to observe how different parameters affect the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We define our base model, dataset details, and create two different training configurations to experiment with. `Config1` is a quick, single-epoch run. `Config2` is a longer, multi-epoch run with a different learning rate and a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Configuration: exp1_lr5e-5_1epoch\n",
      "  - Base Model: Qwen/Qwen3-1.7B-Base\n",
      "  - Output Directory: ./models/exp1_lr5e-5_1epoch\n",
      "  - Epochs: 1\n",
      "  - Learning Rate: 5e-05\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Settings\n",
    "\n",
    "# --- General Settings ---\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen3-1.7B-Base\"\n",
    "DATASET_NAME = \"nuprl/engineering-llm-systems\"\n",
    "DATASET_CONFIG = \"mbpp-rkt-correct-executions\" # Use the training split for SFT\n",
    "WANDB_NOTEBOOK_NAME = \"llm-sft-racket-finetuning\"\n",
    "\n",
    "# --- Experiment 1 Configuration ---\n",
    "class Config1:\n",
    "    RUN_NAME = \"exp1_lr5e-5_1epoch\"\n",
    "    OUTPUT_DIR = f\"./models/{RUN_NAME}\"\n",
    "    NUM_EPOCHS = 1\n",
    "    LEARNING_RATE = 5e-5\n",
    "    BATCH_SIZE = 4\n",
    "    MAX_SEQ_LENGTH = 1024\n",
    "    USE_SCHEDULER = False\n",
    "\n",
    "# --- Experiment 2 Configuration ---\n",
    "class Config2:\n",
    "    RUN_NAME = \"exp2_lr2e-5_3epochs_scheduler\"\n",
    "    OUTPUT_DIR = f\"./models/{RUN_NAME}\"\n",
    "    NUM_EPOCHS = 3\n",
    "    LEARNING_RATE = 2e-5\n",
    "    BATCH_SIZE = 4\n",
    "    MAX_SEQ_LENGTH = 1024\n",
    "    USE_SCHEDULER = True\n",
    "    WARMUP_STEPS = 50\n",
    "\n",
    "\n",
    "# --- SELECT CONFIGURATION TO RUN ---\n",
    "CURRENT_CONFIG = Config2 # Change to Config2 for the second experiment\n",
    "\n",
    "print(f\"Selected Configuration: {CURRENT_CONFIG.RUN_NAME}\")\n",
    "print(f\"  - Base Model: {BASE_MODEL_NAME}\")\n",
    "print(f\"  - Output Directory: {CURRENT_CONFIG.OUTPUT_DIR}\")\n",
    "print(f\"  - Epochs: {CURRENT_CONFIG.NUM_EPOCHS}\")\n",
    "print(f\"  - Learning Rate: {CURRENT_CONFIG.LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B Login\n",
    "\n",
    "You need to log in to your Weights & Biases account to track the experiments. You'll be prompted to enter your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mibarra000\u001b[0m (\u001b[33mibarra000-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Login to Weights & Biases\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model, Tokenizer, and Dataset\n",
    "\n",
    "Here, we load the pre-trained model and tokenizer. We also load the training dataset and define a formatting function. This function creates a single string for each data point, combining the problem description and the solution. This is the text the model will be trained on to learn the task format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✓ Model (Qwen/Qwen3-1.7B-Base) and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Model and Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# Set padding token for batching\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Model ({BASE_MODEL_NAME}) and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded and tokenized. Total examples: 2646\n",
      "\n",
      "Sample decoded tokens from the first example:\n",
      "; Write a function to count the number of vowels in a string, given a specific set of vowels. The function should return the count of characters in the input string that are present in the vowels string.\n",
      "; Input format: The input consists of two lines. The first line is the input string, the second line is the string containing the vowels to check.\n",
      "; Output format: The output is a single integer representing the count of vowels in the input string according to the given vowels.\n",
      "\n",
      "#lang racket\n",
      "\n",
      "#lang racket\n",
      "\n",
      ";; Function to count vowels in a string based on a given set of vowels\n",
      "(define (count-vowels input-string vowels-string)\n",
      "  (define vowels (string->list vowels-string))\n",
      "  (define (is-vowel? char)\n",
      "    (member char vowels))\n",
      "  \n",
      "  (define (count-char char)\n",
      "    (if (is-vowel? char) 1 0))\n",
      "  \n",
      "  (define (count-vowels-in-string str)\n",
      "    (foldl (lambda (char count) (+ count (count-char char))) 0 (string->list str)))\n",
      "  \n",
      "  (count-vowels-in-string input-string))\n",
      "\n",
      ";; Read input from standard input\n",
      "(define input-string (string-downcase (read-line)))\n",
      "(define vowels-string (string-downcase (read-line)))\n",
      "\n",
      ";; Call the function and print the result\n",
      "(display (count-vowels input-string vowels-string))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load and Prepare the Dataset (Corrected for KeyError)\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "train_dataset = load_dataset(DATASET_NAME, DATASET_CONFIG, split=\"train\")\n",
    "\n",
    "# Define a single function to handle both formatting and tokenization for a batch\n",
    "def format_and_tokenize_batch(batch):\n",
    "    \"\"\"\n",
    "    This function takes a batch (a dictionary of lists), formats each example\n",
    "    into a single string, and then tokenizes the list of strings.\n",
    "    \"\"\"\n",
    "    # 1. Create a list of formatted prompt strings from the batch\n",
    "    formatted_texts = []\n",
    "    num_examples = len(batch['description']) # Get the number of items in the batch\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        # Assemble the full text for each individual example\n",
    "        text = (\n",
    "            f\"; {batch['description'][i]}\\n\"\n",
    "            f\"; Input format: {batch['input_format'][i]}\\n\"\n",
    "            f\"; Output format: {batch['output_format'][i]}\\n\\n\"\n",
    "            f\"#lang racket\\n\\n\"\n",
    "            f\"{batch['code'][i]}\" # <--- FIX: Changed 'solution' to 'code'\n",
    "        )\n",
    "        formatted_texts.append(text)\n",
    "\n",
    "    # 2. Tokenize the list of formatted strings\n",
    "    return tokenizer(\n",
    "        formatted_texts,\n",
    "        truncation=True,\n",
    "        max_length=CURRENT_CONFIG.MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# Use .map() to apply the new function efficiently across the dataset\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    format_and_tokenize_batch,\n",
    "    batched=True,\n",
    "    num_proc=4, # Use multiple processes for speed\n",
    "    remove_columns=train_dataset.column_names # Discard old columns\n",
    ")\n",
    "\n",
    "# Set the format to PyTorch tensors for the DataLoader\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(f\"✓ Dataset loaded and tokenized. Total examples: {len(tokenized_dataset)}\")\n",
    "print(\"\\nSample decoded tokens from the first example:\")\n",
    "# Ensure padding tokens are not displayed in the sample output for clarity\n",
    "print(tokenizer.decode(tokenized_dataset[0]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "This is the core of our SFT process. The loop iterates through the specified number of epochs. In each step:\n",
    "\n",
    "1.  We get a batch of data from the `DataLoader`.\n",
    "2.  We perform a **forward pass**. By passing `labels=batch['input_ids']`, the model automatically calculates the causal language modeling loss (cross-entropy loss) for us.\n",
    "3.  We perform a **backward pass** to compute gradients (`loss.backward()`).\n",
    "4.  The optimizer updates the model's weights (`optimizer.step()`).\n",
    "5.  We log the `loss` and `learning_rate` to W&B for monitoring.\n",
    "6.  At the end of each epoch, we save a checkpoint of the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/bchk/eibarra1/wandb/run-20251005_150801-f2sw51sv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning/runs/f2sw51sv' target=\"_blank\">exp1_lr5e-5_1epoch</a></strong> to <a href='https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning' target=\"_blank\">https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning/runs/f2sw51sv' target=\"_blank\">https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning/runs/f2sw51sv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "--- Epoch 1/1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5517f652f64f218dab4d43f2d57cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models/exp1_lr5e-5_1epoch/epoch_1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▅█▆▃▃▃▃▄▂▃▂▃▂▃▄▂▂▂▃▂▁▁▄▁▁▁▂▁▄▁▃▁▁▁▂▃▂▃▁</td></tr><tr><td>step</td><td>▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>loss</td><td>0.03159</td></tr><tr><td>step</td><td>661</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp1_lr5e-5_1epoch</strong> at: <a href='https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning/runs/f2sw51sv' target=\"_blank\">https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning/runs/f2sw51sv</a><br> View project at: <a href='https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning' target=\"_blank\">https://wandb.ai/ibarra000-northeastern-university/llm-sft-racket-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251005_150801-f2sw51sv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Run the Training Process\n",
    "# Modified line to resume training\n",
    "def train(config):\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Initialize W&B run\n",
    "    wandb.init(\n",
    "        project=WANDB_NOTEBOOK_NAME,\n",
    "        name=config.RUN_NAME,\n",
    "        config={\n",
    "            \"model_name\": BASE_MODEL_NAME,\n",
    "            \"dataset\": f\"{DATASET_NAME}/{DATASET_CONFIG}\",\n",
    "            \"learning_rate\": config.LEARNING_RATE,\n",
    "            \"epochs\": config.NUM_EPOCHS,\n",
    "            \"batch_size\": config.BATCH_SIZE,\n",
    "            \"max_seq_length\": config.MAX_SEQ_LENGTH\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    train_loader = DataLoader(tokenized_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    # Learning rate scheduler (optional)\n",
    "    if config.USE_SCHEDULER:\n",
    "        num_training_steps = len(train_loader) * config.NUM_EPOCHS\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=config.WARMUP_STEPS,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    print(\"Starting training...\")\n",
    "    model.train() # Set model to training mode\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{config.NUM_EPOCHS} ---\")\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            # The model computes loss internally when 'labels' are provided\n",
    "            outputs = model(**batch, labels=batch[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if config.USE_SCHEDULER:\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Logging\n",
    "            wandb.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0] if config.USE_SCHEDULER else config.LEARNING_RATE,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"step\": global_step\n",
    "            })\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            global_step += 1\n",
    "        \n",
    "        # --- Save model at the end of each epoch ---\n",
    "        epoch_output_dir = os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}\")\n",
    "        os.makedirs(epoch_output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Saving model checkpoint to {epoch_output_dir}\")\n",
    "        model.save_pretrained(epoch_output_dir)\n",
    "        tokenizer.save_pretrained(epoch_output_dir)\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# Run training with the selected configuration\n",
    "train(CURRENT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shared-bchk-venv",
   "language": "python",
   "name": "shared-bchk-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
