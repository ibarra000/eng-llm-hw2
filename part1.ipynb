{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29142c85-e004-40c8-b026-2f051f2bfb74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 2: Import Libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import subprocess\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3aaf982-9446-4dba-9e34-d44713e0bed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Cell 3: Define Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load the pre-trained model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "    \n",
    "    Returns:\n",
    "        model: The loaded language model\n",
    "        tokenizer: The corresponding tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Determine device (use GPU if available, otherwise CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model without device_map (which requires accelerate)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"✓ Model and tokenizer loaded successfully\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_test_dataset():\n",
    "    \"\"\"\n",
    "    Load the test dataset for Racket code generation.\n",
    "    \n",
    "    Returns:\n",
    "        dataset: The test problems dataset (the 'train' split)\n",
    "    \"\"\"\n",
    "    print(\"Loading test dataset...\")\n",
    "    dataset = load_dataset(\n",
    "        \"nuprl/engineering-llm-systems\",\n",
    "        \"mbpp-rkt-test-problems\"\n",
    "    )\n",
    "    # The dataset structure uses 'train' as the split name\n",
    "    test_data = dataset['train']\n",
    "    print(f\"✓ Loaded {len(test_data)} test problems\")\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def generate_completions(model, tokenizer, prompt, num_completions=5):\n",
    "    \"\"\"\n",
    "    Generate multiple completions for a given prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The input prompt text\n",
    "        num_completions: Number of completions to generate (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        completions: List of generated code strings\n",
    "    \"\"\"\n",
    "    # Set pad token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize the input prompt with attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    completions = []\n",
    "    \n",
    "    # Generate completions one at a time to avoid parallelism\n",
    "    for i in range(num_completions):\n",
    "        print(f\"  Generating completion {i+1}/{num_completions}...\", end='\\r')\n",
    "        \n",
    "        # Generate with specified parameters, including attention_mask\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=512,\n",
    "            top_p=0.95,\n",
    "            temperature=0.2,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode the generated tokens\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the completion (remove the prompt part)\n",
    "        completion = generated_text[len(prompt):]\n",
    "        completions.append(completion)\n",
    "    \n",
    "    print(f\"  ✓ Generated {num_completions} completions\")\n",
    "    return completions\n",
    "\n",
    "\n",
    "def save_generation_to_json(task_id, problem, prompt, completions, output_dir=\"completions\"):\n",
    "    \"\"\"\n",
    "    Save generation information to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        task_id: Problem task ID\n",
    "        problem: The problem dictionary\n",
    "        prompt: The prompt used for generation\n",
    "        completions: List of generated completions\n",
    "        output_dir: Directory to save JSON files\n",
    "    \n",
    "    Returns:\n",
    "        filepath: Path to the saved JSON file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Prepare data structure\n",
    "    generation_data = {\n",
    "        'task_id': task_id,\n",
    "        'description': problem['description'],\n",
    "        'input_format': problem['input_format'],\n",
    "        'output_format': problem['output_format'],\n",
    "        'prompt': prompt,\n",
    "        'completions': completions,\n",
    "        'test_cases': problem['tests']\n",
    "    }\n",
    "    \n",
    "    # Save to JSON file\n",
    "    filepath = os.path.join(output_dir, f\"task_{task_id}.json\")\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(generation_data, f, indent=2)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "\n",
    "def generate_and_save_all_completions(model, tokenizer, dataset, output_dir=\"completions\"):\n",
    "    \"\"\"\n",
    "    Generate completions for all problems and save to JSON files.\n",
    "    Skips problems that already have saved completions.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        dataset: The test dataset\n",
    "        output_dir: Directory to save JSON files\n",
    "    \n",
    "    Returns:\n",
    "        num_generated: Number of problems actually generated (not skipped)\n",
    "    \"\"\"\n",
    "    total_problems = len(dataset)\n",
    "    num_generated = 0\n",
    "    num_skipped = 0\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generating completions for {total_problems} problems\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx, problem in enumerate(dataset):\n",
    "        task_id = problem['task_id']\n",
    "        \n",
    "        # Check if file already exists\n",
    "        filepath = os.path.join(output_dir, f\"task_{task_id}.json\")\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"[{idx+1}/{total_problems}] Task {task_id} - ⏭️  SKIPPED (already exists)\")\n",
    "            num_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"[{idx+1}/{total_problems}] Task {task_id}\")\n",
    "        \n",
    "        # Create prompt\n",
    "        description = problem['description']\n",
    "        input_format = problem['input_format']\n",
    "        output_format = problem['output_format']\n",
    "        \n",
    "        prompt = f\"\"\"; {description}\n",
    "; Input format: {input_format}\n",
    "; Output format: {output_format}\n",
    "\n",
    "#lang racket\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate completions\n",
    "        completions = generate_completions(model, tokenizer, prompt, num_completions=5)\n",
    "        \n",
    "        # Save to JSON\n",
    "        filepath = save_generation_to_json(task_id, problem, prompt, completions, output_dir)\n",
    "        print(f\"  ✓ Saved to {filepath}\\n\")\n",
    "        num_generated += 1\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Generation complete!\")\n",
    "    print(f\"  Generated: {num_generated} new problems\")\n",
    "    print(f\"  Skipped: {num_skipped} existing problems\")\n",
    "    print(f\"  Total files in '{output_dir}/': {num_generated + num_skipped}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return num_generated\n",
    "\n",
    "\n",
    "def test_racket_completion(completion_string, test_cases, racket_path=None):\n",
    "    \"\"\"\n",
    "    Tests a Racket code completion against a list of test cases.\n",
    "    \n",
    "    Args:\n",
    "        completion_string: A string containing the entire Racket program\n",
    "        test_cases: A list of dictionaries, where each dict has 'input' and 'output' keys\n",
    "        racket_path: Path to Racket executable (uses RACKET_PATH global if None)\n",
    "    \n",
    "    Returns:\n",
    "        A list of dictionaries detailing the results for each test case\n",
    "    \"\"\"\n",
    "    # Use global RACKET_PATH if not specified\n",
    "    if racket_path is None:\n",
    "        racket_path = globals().get('RACKET_PATH', 'racket')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Use a temporary file to store the Racket code\n",
    "    # The 'with' statement handles cleanup automatically\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        mode='w+', suffix='.rkt', delete=False\n",
    "    ) as temp_file:\n",
    "        temp_file.write(completion_string)\n",
    "        temp_file_path = temp_file.name\n",
    "    \n",
    "    try:\n",
    "        for case in test_cases:\n",
    "            test_input = case['input']\n",
    "            expected_output = case['output']\n",
    "            \n",
    "            try:\n",
    "                # Execute the racket file as a subprocess\n",
    "                # We set a timeout of 5 seconds to prevent infinite loops\n",
    "                process = subprocess.run(\n",
    "                    [racket_path, temp_file_path],\n",
    "                    input=test_input,\n",
    "                    capture_output=True,\n",
    "                    text=True,  # Work with strings, not bytes\n",
    "                    timeout=5\n",
    "                )\n",
    "                \n",
    "                # The Racket code ran but might have crashed\n",
    "                if process.returncode != 0:\n",
    "                    actual_output = f\"RUNTIME ERROR:\\n{process.stderr.strip()}\"\n",
    "                    passed = False\n",
    "                else:\n",
    "                    # The code ran successfully, compare its output\n",
    "                    actual_output = process.stdout.strip()\n",
    "                    passed = (actual_output == expected_output)\n",
    "                    \n",
    "            except subprocess.TimeoutExpired:\n",
    "                actual_output = \"EXECUTION TIMED OUT (5 seconds)\"\n",
    "                passed = False\n",
    "                \n",
    "            results.append({\n",
    "                'passed': passed,\n",
    "                'input': test_input,\n",
    "                'expected': expected_output,\n",
    "                'actual': actual_output\n",
    "            })\n",
    "            \n",
    "    finally:\n",
    "        # Ensure the temporary file is deleted even if errors occur\n",
    "        os.remove(temp_file_path)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def check_test_cases(test_results):\n",
    "    \"\"\"\n",
    "    Check if all test cases passed.\n",
    "    \n",
    "    Args:\n",
    "        test_results: List of test result dictionaries from test_racket_completion\n",
    "    \n",
    "    Returns:\n",
    "        passed: Boolean indicating if all tests passed\n",
    "    \"\"\"\n",
    "    return all(result['passed'] for result in test_results)\n",
    "\n",
    "\n",
    "def evaluate_from_json(json_filepath):\n",
    "    \"\"\"\n",
    "    Load a generation JSON file and evaluate all completions.\n",
    "    \n",
    "    Args:\n",
    "        json_filepath: Path to the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        passed: Boolean indicating if at least one completion passed\n",
    "        results: Dictionary with detailed results for each completion\n",
    "    \"\"\"\n",
    "    # Load the generation data\n",
    "    with open(json_filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    completions = data['completions']\n",
    "    test_cases = data['test_cases']\n",
    "    \n",
    "    results = {\n",
    "        'task_id': data['task_id'],\n",
    "        'completions_tested': [],\n",
    "        'any_passed': False\n",
    "    }\n",
    "    \n",
    "    # Test each completion\n",
    "    for i, completion in enumerate(completions):\n",
    "        # Combine prompt and completion\n",
    "        full_code = prompt + completion\n",
    "        \n",
    "        # Test the code using the new test function\n",
    "        test_results = test_racket_completion(full_code, test_cases)\n",
    "        \n",
    "        # Check if all tests passed\n",
    "        all_tests_passed = check_test_cases(test_results)\n",
    "        \n",
    "        completion_result = {\n",
    "            'completion_index': i,\n",
    "            'all_tests_passed': all_tests_passed,\n",
    "            'test_details': test_results\n",
    "        }\n",
    "        \n",
    "        if all_tests_passed:\n",
    "            results['any_passed'] = True\n",
    "        \n",
    "        results['completions_tested'].append(completion_result)\n",
    "    \n",
    "    return results['any_passed'], results\n",
    "\n",
    "\n",
    "def evaluate_all_completions(completions_dir=\"completions\"):\n",
    "    \"\"\"\n",
    "    Evaluate all generated completions from JSON files.\n",
    "    \n",
    "    Args:\n",
    "        completions_dir: Directory containing JSON files\n",
    "    \n",
    "    Returns:\n",
    "        pass_at_1: The pass@1 score\n",
    "        all_results: List of detailed results for each problem\n",
    "    \"\"\"\n",
    "    # Get all JSON files\n",
    "    json_files = sorted(Path(completions_dir).glob(\"task_*.json\"))\n",
    "    total_problems = len(json_files)\n",
    "    \n",
    "    if total_problems == 0:\n",
    "        print(f\"No JSON files found in '{completions_dir}/'\")\n",
    "        return 0.0, []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {total_problems} problems from '{completions_dir}/'\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    problems_passed = 0\n",
    "    all_results = []\n",
    "    \n",
    "    # Evaluate each problem\n",
    "    for idx, json_file in enumerate(json_files):\n",
    "        print(f\"[{idx+1}/{total_problems}] Evaluating {json_file.name}\")\n",
    "        \n",
    "        passed, detailed_results = evaluate_from_json(json_file)\n",
    "        \n",
    "        if passed:\n",
    "            problems_passed += 1\n",
    "            print(f\"  ✓ PASSED\\n\")\n",
    "        else:\n",
    "            print(f\"  ✗ FAILED\\n\")\n",
    "        \n",
    "        all_results.append(detailed_results)\n",
    "    \n",
    "    # Calculate pass@1 score\n",
    "    pass_at_1 = problems_passed / total_problems\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Problems passed: {problems_passed}/{total_problems}\")\n",
    "    print(f\"Pass@1 Score: {pass_at_1:.4f} ({pass_at_1*100:.2f}%)\")\n",
    "    \n",
    "    return pass_at_1, all_results\n",
    "\n",
    "\n",
    "def save_evaluation_results(pass_at_1, all_results, output_file=\"evaluation_results.json\"):\n",
    "    \"\"\"\n",
    "    Save evaluation results to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        pass_at_1: The pass@1 score\n",
    "        all_results: List of detailed results\n",
    "        output_file: Output filename\n",
    "    \"\"\"\n",
    "    results_summary = {\n",
    "        'pass_at_1_score': pass_at_1,\n",
    "        'total_problems': len(all_results),\n",
    "        'problems_passed': sum(1 for r in all_results if r['any_passed']),\n",
    "        'detailed_results': all_results\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Detailed results saved to {output_file}\")\n",
    "\n",
    "print(\"✓ All functions defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ef485b-03a1-4a5c-ba59-a1f05e85018f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set:\n",
      "  Model: Qwen/Qwen3-1.7B-Base\n",
      "  Completions directory: completions\n",
      "  Racket path: /u/eibarra1/racket/bin/racket\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Cell 4: Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-1.7B-Base\"\n",
    "COMPLETIONS_DIR = \"completions\"\n",
    "\n",
    "# Racket executable path\n",
    "RACKET_PATH = os.path.expanduser(\"~/racket/bin/racket\")\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Completions directory: {COMPLETIONS_DIR}\")\n",
    "print(f\"  Racket path: {RACKET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8440c549-f29c-450f-bb36-118b2461598c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen3-1.7B-Base\n",
      "Using device: cuda\n",
      "✓ Model and tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Cell 5: Load Model and Tokenizer\n",
    "# ============================================================================\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d361a458-3da5-4074-aaef-2ce49886e543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "✓ Loaded 50 test problems\n",
      "\n",
      "Sample problem from dataset:\n",
      "{'description': 'Given a list of lists, write a function to find the list with the maximum length using a lambda function. Return a tuple containing the length of the longest list and the list itself.', 'input_format': 'The first line contains an integer N, the number of lists. This is followed by N lines, each containing space-separated integers representing a list.', 'output_format': 'The output is the length of the longest list followed by the elements of the longest list, all separated by spaces.', 'tests': [{'input': '5\\n0\\n1 3\\n5 7\\n9 11\\n13 15 17', 'output': '3 13 15 17'}, {'input': '5\\n1 2 3 4 5\\n1 2 3 4\\n1 2 3\\n1 2\\n1', 'output': '5 1 2 3 4 5'}, {'input': '3\\n3 4 5\\n6 7 8 9\\n10 11 12', 'output': '4 6 7 8 9'}], 'task_id': 393}\n",
      "Task ID: 393\n",
      "Description: Given a list of lists, write a function to find the list with the maximum length using a lambda function. Return a tuple containing the length of the longest list and the list itself.\n",
      "Number of tests: 3\n",
      "First test input preview: 5\n",
      "0\n",
      "1 3\n",
      "5 7\n",
      "9 11\n",
      "13 15 17...\n",
      "First test expected output: 3 13 15 17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Cell 6: Load Test Dataset\n",
    "# ============================================================================\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = load_test_dataset()\n",
    "\n",
    "# Display sample problem\n",
    "print(\"\\nSample problem from dataset:\")\n",
    "sample = test_dataset[0]\n",
    "print(sample)\n",
    "print(f\"Task ID: {sample['task_id']}\")\n",
    "print(f\"Description: {sample['description']}\")\n",
    "print(f\"Number of tests: {len(sample['tests'])}\")\n",
    "print(f\"First test input preview: {sample['tests'][0]['input'][:100]}...\")\n",
    "print(f\"First test expected output: {sample['tests'][0]['output']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7c586c-8892-4377-8160-0434a2f07ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation... This may take a while!\n",
      "\n",
      "\n",
      "============================================================\n",
      "Generating completions for 50 problems\n",
      "============================================================\n",
      "\n",
      "[1/50] Task 393 - ⏭️  SKIPPED (already exists)\n",
      "[2/50] Task 71 - ⏭️  SKIPPED (already exists)\n",
      "[3/50] Task 97 - ⏭️  SKIPPED (already exists)\n",
      "[4/50] Task 353 - ⏭️  SKIPPED (already exists)\n",
      "[5/50] Task 307 - ⏭️  SKIPPED (already exists)\n",
      "[6/50] Task 64 - ⏭️  SKIPPED (already exists)\n",
      "[7/50] Task 445 - ⏭️  SKIPPED (already exists)\n",
      "[8/50] Task 205 - ⏭️  SKIPPED (already exists)\n",
      "[9/50] Task 333 - ⏭️  SKIPPED (already exists)\n",
      "[10/50] Task 498 - ⏭️  SKIPPED (already exists)\n",
      "[11/50] Task 178 - ⏭️  SKIPPED (already exists)\n",
      "[12/50] Task 342 - ⏭️  SKIPPED (already exists)\n",
      "[13/50] Task 268 - ⏭️  SKIPPED (already exists)\n",
      "[14/50] Task 51 - ⏭️  SKIPPED (already exists)\n",
      "[15/50] Task 484 - ⏭️  SKIPPED (already exists)\n",
      "[16/50] Task 322 - ⏭️  SKIPPED (already exists)\n",
      "[17/50] Task 443 - ⏭️  SKIPPED (already exists)\n",
      "[18/50] Task 464 - ⏭️  SKIPPED (already exists)\n",
      "[19/50] Task 197 - ⏭️  SKIPPED (already exists)\n",
      "[20/50] Task 347 - ⏭️  SKIPPED (already exists)\n",
      "[21/50] Task 477 - ⏭️  SKIPPED (already exists)\n",
      "[22/50] Task 315 - ⏭️  SKIPPED (already exists)\n",
      "[23/50] Task 75 - ⏭️  SKIPPED (already exists)\n",
      "[24/50] Task 398 - ⏭️  SKIPPED (already exists)\n",
      "[25/50] Task 354 - ⏭️  SKIPPED (already exists)\n",
      "[26/50] Task 137 - ⏭️  SKIPPED (already exists)\n",
      "[27/50] Task 476 - ⏭️  SKIPPED (already exists)\n",
      "[28/50] Task 491 - ⏭️  SKIPPED (already exists)\n",
      "[29/50] Task 42 - ⏭️  SKIPPED (already exists)\n",
      "[30/50] Task 459 - ⏭️  SKIPPED (already exists)\n",
      "[31/50] Task 265\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_265.json\n",
      "\n",
      "[32/50] Task 180\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_180.json\n",
      "\n",
      "[33/50] Task 84\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_84.json\n",
      "\n",
      "[34/50] Task 507\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_507.json\n",
      "\n",
      "[35/50] Task 256\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_256.json\n",
      "\n",
      "[36/50] Task 458\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_458.json\n",
      "\n",
      "[37/50] Task 351\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_351.json\n",
      "\n",
      "[38/50] Task 479\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_479.json\n",
      "\n",
      "[39/50] Task 135\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_135.json\n",
      "\n",
      "[40/50] Task 81\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_81.json\n",
      "\n",
      "[41/50] Task 352\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_352.json\n",
      "\n",
      "[42/50] Task 65\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_65.json\n",
      "\n",
      "[43/50] Task 130\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_130.json\n",
      "\n",
      "[44/50] Task 193\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_193.json\n",
      "\n",
      "[45/50] Task 497\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_497.json\n",
      "\n",
      "[46/50] Task 72\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_72.json\n",
      "\n",
      "[47/50] Task 102\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_102.json\n",
      "\n",
      "[48/50] Task 449\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_449.json\n",
      "\n",
      "[49/50] Task 372\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_372.json\n",
      "\n",
      "[50/50] Task 329\n",
      "  ✓ Generated 5 completions...\n",
      "  ✓ Saved to completions/task_329.json\n",
      "\n",
      "============================================================\n",
      "Generation complete!\n",
      "  Generated: 20 new problems\n",
      "  Skipped: 30 existing problems\n",
      "  Total files in 'completions/': 50\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Generated and saved completions for 20 problems\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Cell 7: Generate Completions and Save to JSON (This will take time!)\n",
    "# ============================================================================\n",
    "\n",
    "# Generate completions for all problems and save to JSON\n",
    "print(\"Starting generation... This may take a while!\\n\")\n",
    "num_generated = generate_and_save_all_completions(model, tokenizer, test_dataset, COMPLETIONS_DIR)\n",
    "\n",
    "print(f\"\\n✓ Generated and saved completions for {num_generated} problems\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5deb17-e8c2-45b8-95dd-6b68a0f083f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation from saved JSON files...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Evaluating 50 problems from 'completions/'\n",
      "============================================================\n",
      "\n",
      "[1/50] Evaluating task_102.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[2/50] Evaluating task_130.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[3/50] Evaluating task_135.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[4/50] Evaluating task_137.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[5/50] Evaluating task_178.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[6/50] Evaluating task_180.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[7/50] Evaluating task_193.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[8/50] Evaluating task_197.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[9/50] Evaluating task_205.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[10/50] Evaluating task_256.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[11/50] Evaluating task_265.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[12/50] Evaluating task_268.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[13/50] Evaluating task_307.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[14/50] Evaluating task_315.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[15/50] Evaluating task_322.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[16/50] Evaluating task_329.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[17/50] Evaluating task_333.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[18/50] Evaluating task_342.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[19/50] Evaluating task_347.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[20/50] Evaluating task_351.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[21/50] Evaluating task_352.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[22/50] Evaluating task_353.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[23/50] Evaluating task_354.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[24/50] Evaluating task_372.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[25/50] Evaluating task_393.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[26/50] Evaluating task_398.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[27/50] Evaluating task_42.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[28/50] Evaluating task_443.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[29/50] Evaluating task_445.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[30/50] Evaluating task_449.json\n",
      "  ✓ PASSED\n",
      "\n",
      "[31/50] Evaluating task_458.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[32/50] Evaluating task_459.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[33/50] Evaluating task_464.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[34/50] Evaluating task_476.json\n",
      "  ✓ PASSED\n",
      "\n",
      "[35/50] Evaluating task_477.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[36/50] Evaluating task_479.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[37/50] Evaluating task_484.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[38/50] Evaluating task_491.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[39/50] Evaluating task_497.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[40/50] Evaluating task_498.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[41/50] Evaluating task_507.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[42/50] Evaluating task_51.json\n",
      "  ✓ PASSED\n",
      "\n",
      "[43/50] Evaluating task_64.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[44/50] Evaluating task_65.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[45/50] Evaluating task_71.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[46/50] Evaluating task_72.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[47/50] Evaluating task_75.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[48/50] Evaluating task_81.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[49/50] Evaluating task_84.json\n",
      "  ✗ FAILED\n",
      "\n",
      "[50/50] Evaluating task_97.json\n",
      "  ✗ FAILED\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE\n",
      "============================================================\n",
      "Problems passed: 3/50\n",
      "Pass@1 Score: 0.0600 (6.00%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Cell 8: Evaluate Completions from JSON Files\n",
    "# ============================================================================\n",
    "\n",
    "# Evaluate all completions from JSON files\n",
    "print(\"Starting evaluation from saved JSON files...\\n\")\n",
    "pass_at_1_score, evaluation_results = evaluate_all_completions(COMPLETIONS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fceb9-49fe-474d-81b1-0762213f9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Cell 9: Save and Display Results\n",
    "# ============================================================================\n",
    "\n",
    "# Save detailed evaluation results\n",
    "save_evaluation_results(pass_at_1_score, evaluation_results, \"evaluation_results.json\")\n",
    "\n",
    "# Save summary to text file\n",
    "with open('baseline_results.txt', 'w') as f:\n",
    "    f.write(f\"Model: {MODEL_NAME}\\n\")\n",
    "    f.write(f\"Pass@1 Score: {pass_at_1_score:.4f}\\n\")\n",
    "    f.write(f\"\\nDetailed Results:\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    for result in evaluation_results:\n",
    "        status = \"PASSED\" if result['any_passed'] else \"FAILED\"\n",
    "        f.write(f\"Task {result['task_id']}: {status}\\n\")\n",
    "\n",
    "print(\"✓ Results saved to baseline_results.txt\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Total Problems: {len(evaluation_results)}\")\n",
    "print(f\"Problems Passed: {sum(1 for r in evaluation_results if r['any_passed'])}\")\n",
    "print(f\"Pass@1 Score: {pass_at_1_score:.4f} ({pass_at_1_score*100:.2f}%)\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shared-bchk-venv",
   "language": "python",
   "name": "shared-bchk-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
